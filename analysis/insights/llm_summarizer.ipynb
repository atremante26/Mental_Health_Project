{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7947564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from load_data import load_dataset\n",
    "import google.generativeai as genai\n",
    "from insights.prompt_templates import *\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df96540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "print(\"Gemini model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b3144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reddit Data\n",
    "reddit_df = load_dataset('reddit')\n",
    "print(f\"Loaded {len(reddit_df)} Reddit posts\")\n",
    "print(f\"Columns: {list(reddit_df.columns)}\")\n",
    "\n",
    "# Sample the data to see text content\n",
    "print(\"\\nSample post:\")\n",
    "print(reddit_df[['title', 'text', 'subreddit', 'score']].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_reddit_sample(df, max_posts=20, max_length=200):\n",
    "    \"\"\"Prepare a sample of Reddit posts for LLM analysis\"\"\"\n",
    "    # Sample posts to stay within API limits\n",
    "    sample_df = df.sample(n=min(len(df), max_posts))\n",
    "    \n",
    "    posts_text = []\n",
    "    for _, row in sample_df.iterrows():\n",
    "        # Truncate long posts to avoid token limits\n",
    "        title = row['title'][:100]\n",
    "        text = str(row['text'])[:max_length] if pd.notna(row['text']) else \"\"\n",
    "        subreddit = row['subreddit']\n",
    "        \n",
    "        post_summary = f\"Subreddit: {subreddit}\\nTitle: {title}\\nText: {text}\\n---\"\n",
    "        posts_text.append(post_summary)\n",
    "    \n",
    "    return \"\\n\".join(posts_text)\n",
    "\n",
    "# Prepare sample\n",
    "reddit_sample = prepare_reddit_sample(reddit_df)\n",
    "print(f\"Prepared sample with {len(reddit_sample.split('---'))-1} posts\")\n",
    "print(f\"Sample length: {len(reddit_sample)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08522c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_reddit_sentiment(posts_text):\n",
    "    \"\"\"Analyze Reddit posts using Gemini\"\"\"\n",
    "    try:\n",
    "        prompt = REDDIT_SENTIMENT_PROMPT.format(posts_text=posts_text)\n",
    "        \n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        return response.text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini API: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run analysis\n",
    "print(\"Analyzing Reddit posts with Gemini...\")\n",
    "sentiment_analysis = analyze_reddit_sentiment(reddit_sample)\n",
    "\n",
    "if sentiment_analysis:\n",
    "    print(\"Analysis Results:\")\n",
    "    print(sentiment_analysis)\n",
    "else:\n",
    "    print(\"Analysis failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sentiment_analysis:\n",
    "    # Save analysis to outputs\n",
    "    os.makedirs('../outputs/reports', exist_ok=True)\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'sample_size': len(reddit_sample.split('---'))-1,\n",
    "        'analysis': sentiment_analysis\n",
    "    }\n",
    "    \n",
    "    filename = f\"../outputs/reports/reddit_sentiment_{datetime.now().strftime('%Y%m%d')}.json\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"Analysis saved to: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
