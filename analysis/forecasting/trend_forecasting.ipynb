{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4012a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:prophet.plot:Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from analysis.load_data import load_dataset\n",
    "from analysis.utils.preprocessing import prepare_time_series, prepare_who_time_series\n",
    "from analysis.config.model_config import FORECASTING_CONFIG\n",
    "from analysis.forecasting.forecasting_utils import train_prophet_model, evaluate_forecast_model, save_forecast_results, cross_validate_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b92c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.16.0, Python Version: 3.11.13, Platform: macOS-15.4.1-x86_64-i386-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "/Users/Andrew/Desktop/Computer Science/Mental_Health_Project/analysis/load_data.py:43: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "ERROR:analysis.load_data:Error executing query from /Users/Andrew/Desktop/Computer Science/Mental_Health_Project/pipeline/snowflake/cdc_sql/cdc_extract.sql: Execution failed on sql '-- Use CDC Schema\n",
      "USE SCHEMA MENTAL_HEALTH.CDC;\n",
      "\n",
      "-- Select all CDC data\n",
      "SELECT * FROM CDC_PROCESSED;': 000008 (0A000): 01bf7e76-0000-376b-0024-9b8b0016f006: Actual statement count 2 did not match the desired statement count 1.\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.16.0, Python Version: 3.11.13, Platform: macOS-15.4.1-x86_64-i386-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "ERROR:analysis.load_data:Error executing query from /Users/Andrew/Desktop/Computer Science/Mental_Health_Project/pipeline/snowflake/trends_sql/trends_extract.sql: Execution failed on sql '-- Use CDC Schema\n",
      "USE SCHEMA MENTAL_HEALTH.GOOGLE_TRENDS;\n",
      "\n",
      "-- Select all CDC data\n",
      "SELECT * FROM GOOGLE_TRENDS_PROCESSED;': 000008 (0A000): 01bf7e76-0000-3699-0024-9b8b0016a01a: Actual statement count 2 did not match the desired statement count 1.\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.16.0, Python Version: 3.11.13, Platform: macOS-15.4.1-x86_64-i386-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "ERROR:analysis.load_data:Error executing query from /Users/Andrew/Desktop/Computer Science/Mental_Health_Project/pipeline/snowflake/static/who_suicide_extract.sql: Execution failed on sql '-- Use CDC Schema\n",
      "USE SCHEMA MENTAL_HEALTH.STATIC;\n",
      "\n",
      "-- Select all CDC data\n",
      "SELECT * FROM WHO_SUICIDE;': 000008 (0A000): 01bf7e76-0000-376b-0024-9b8b0016f00a: Actual statement count 2 did not match the desired statement count 1.\n"
     ]
    }
   ],
   "source": [
    "# Load CDC data\n",
    "cdc_df = load_dataset('cdc')\n",
    "\n",
    "# Load Google Trends data\n",
    "trends_df = load_dataset('trends')  \n",
    "\n",
    "# Load WHO suicide data\n",
    "who_suicide_df = load_dataset('who_suicide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b463a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare CDC anxiety data\n",
    "cdc_ts = prepare_time_series(cdc_df, 'date', 'anxiety')\n",
    "\n",
    "# Prepare Google Trends data\n",
    "trends_ts = prepare_time_series(trends_df, 'date', 'interest')\n",
    "\n",
    "# Prepare WHO suicide data\n",
    "who_suicide_ts = prepare_who_time_series(who_suicide_df, 'year', 'suicides_no')\n",
    "\n",
    "# Convert all to Prophet format\n",
    "datasets_to_forecast = {}\n",
    "\n",
    "# CDC data\n",
    "cdc_prophet = cdc_ts.reset_index()\n",
    "cdc_prophet.columns = ['ds', 'y']\n",
    "datasets_to_forecast['cdc_anxiety'] = cdc_prophet\n",
    "\n",
    "# Google Trends data\n",
    "trends_prophet = trends_ts.reset_index()\n",
    "trends_prophet.columns = ['ds', 'y']\n",
    "datasets_to_forecast['google_trends'] = trends_prophet\n",
    "\n",
    "# WHO data\n",
    "who_prophet = who_suicide_ts.reset_index()\n",
    "who_prophet.columns = ['ds', 'y']\n",
    "datasets_to_forecast['who_suicides'] = who_prophet\n",
    "\n",
    "# Plot all time series for comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "for i, (name, data) in enumerate(datasets_to_forecast.items()):\n",
    "    axes[i].plot(data['ds'], data['y'])\n",
    "    axes[i].set_title(f'{name.replace(\"_\", \" \").title()} Over Time')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Prophet model\n",
    "config = FORECASTING_CONFIG['prophet']\n",
    "forecast_results = {}\n",
    "\n",
    "for dataset_name, prophet_data in datasets_to_forecast.items():\n",
    "    # Train model using utility function\n",
    "    model = train_prophet_model(prophet_data, config)\n",
    "    \n",
    "    # Generate forecast (different periods based on data frequency)\n",
    "    if dataset_name == 'who_suicides':\n",
    "        periods = 5  # 5 years ahead for annual data\n",
    "    else:\n",
    "        periods = 90  # 90 days ahead for higher frequency data\n",
    "    \n",
    "    future = model.make_future_dataframe(periods=periods)\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # Store results\n",
    "    forecast_results[dataset_name] = {\n",
    "        'model': model,\n",
    "        'forecast': forecast,\n",
    "        'training_data': prophet_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all forecasts\n",
    "fig, axes = plt.subplots(len(forecast_results), 2, figsize=(20, 6*len(forecast_results)))\n",
    "\n",
    "for i, (dataset_name, results) in enumerate(forecast_results.items()):\n",
    "    model = results['model']\n",
    "    forecast = results['forecast']\n",
    "    \n",
    "    # Main forecast plot\n",
    "    fig1 = model.plot(forecast, ax=axes[i,0])\n",
    "    axes[i,0].set_title(f'{dataset_name.replace(\"_\", \" \").title()} Forecast')\n",
    "    axes[i,0].set_ylabel('Value')\n",
    "    \n",
    "    # Components plot\n",
    "    fig2 = model.plot_components(forecast, ax=axes[i,1] if len(forecast_results) == 1 else None)\n",
    "    if len(forecast_results) > 1:\n",
    "        # For multiple subplots, create separate components plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        model.plot_components(forecast)\n",
    "        plt.suptitle(f'{dataset_name.replace(\"_\", \" \").title()} Components')\n",
    "        plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d969558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "evaluation_results = {}\n",
    "\n",
    "for dataset_name, results in forecast_results.items():\n",
    "    training_data = results['training_data']\n",
    "    \n",
    "    if len(training_data) > 30:\n",
    "        # Split data for validation\n",
    "        train_size = int(len(training_data) * 0.8)\n",
    "        train_data = training_data[:train_size]\n",
    "        test_data = training_data[train_size:]\n",
    "        \n",
    "        # Train evaluation model\n",
    "        eval_model = train_prophet_model(train_data, config)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        metrics = evaluate_forecast_model(eval_model, train_data, test_data, model_type='prophet')\n",
    "        evaluation_results[dataset_name] = metrics\n",
    "        \n",
    "        print(\"Model Evaluation Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric.upper()}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"Insufficient data for evaluation ({len(training_data)} points)\")\n",
    "        evaluation_results[dataset_name] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models/results\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "for dataset_name, results in forecast_results.items():\n",
    "    model = results['model'] \n",
    "    forecast = results['forecast']\n",
    "    \n",
    "    filename = f'{dataset_name}_{timestamp}'\n",
    "    save_forecast_results(model, forecast, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "cv_results = {}\n",
    "\n",
    "for dataset_name, results in forecast_results.items():\n",
    "    training_data = results['training_data']\n",
    "\n",
    "    # Adjust minimum data requirements based on frequency\n",
    "    min_required = 365 if dataset_name != 'who_suicides' else 15  # 15 years for annual data\n",
    "    \n",
    "    if len(training_data) > min_required:\n",
    "        try:\n",
    "            cv_output, cv_metrics = cross_validate_timeseries(training_data, config)\n",
    "            cv_results[dataset_name] = cv_metrics\n",
    "            \n",
    "            print(\"Cross-validation metrics summary:\")\n",
    "            print(cv_metrics[['mape', 'rmse']].describe())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Cross-validation failed: {e}\")\n",
    "            cv_results[dataset_name] = None\n",
    "    else:\n",
    "        print(f\"Need more data for cross-validation ({len(training_data)}/{min_required} points)\")\n",
    "        cv_results[dataset_name] = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
