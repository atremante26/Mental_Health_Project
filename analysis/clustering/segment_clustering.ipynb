{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529f1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from analysis.load_data import load_dataset\n",
    "from analysis.utils.preprocessing import prepare_clustering_features, compute_gower_distance\n",
    "from analysis.clustering.clustering_utils import run_hdbscan_clustering, evaluate_clustering\n",
    "from analysis.config.model_config import CLUSTERING_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1ff39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data.PROJECT_ROOT = /Users/Andrew/Desktop/Computer Science/Mental_Health_Project\n",
      "Resolves to: /Users/Andrew/Desktop/Computer Science/Mental_Health_Project\n",
      "SQL_PATHS['tech_survey'] = pipeline/snowflake/static/tech_survey_extract.sql\n",
      "Full path = /Users/Andrew/Desktop/Computer Science/Mental_Health_Project/pipeline/snowflake/static/tech_survey_extract.sql\n",
      "That path exists: True\n"
     ]
    }
   ],
   "source": [
    "import analysis.load_data as ld\n",
    "print(f\"load_data.PROJECT_ROOT = {ld.PROJECT_ROOT}\")\n",
    "print(f\"Resolves to: {ld.PROJECT_ROOT.resolve()}\")\n",
    "\n",
    "# Check what SQL path it's building\n",
    "from analysis.config.data_paths import SQL_PATHS\n",
    "print(f\"SQL_PATHS['tech_survey'] = {SQL_PATHS['tech_survey']}\")\n",
    "print(f\"Full path = {ld.PROJECT_ROOT / SQL_PATHS['tech_survey']}\")\n",
    "print(f\"That path exists: {(ld.PROJECT_ROOT / SQL_PATHS['tech_survey']).exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a6940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.16.0, Python Version: 3.11.13, Platform: macOS-15.4.1-x86_64-i386-64bit\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - PROJECT_ROOT: /Users/Andrew/Desktop/Computer Science/Mental_Health_Project\n",
      "DEBUG - SQL_PATHS[tech_survey]: pipeline/snowflake/static/tech_survey_extract.sql\n",
      "DEBUG - Resolved sql_path: /Users/Andrew/Desktop/Computer Science/Mental_Health_Project/pipeline/snowflake/static/tech_survey_extract.sql\n",
      "DEBUG - File exists: True\n",
      "DEBUG - Is file: True\n",
      "DEBUG - load_from_sql_file called\n",
      "DEBUG - Opening file: /Users/Andrew/Desktop/Computer Science/Mental_Health_Project/pipeline/snowflake/static/tech_survey_extract.sql\n",
      "DEBUG - Read 47 characters from SQL file\n",
      "DEBUG - Connecting to Snowflake...\n",
      "DEBUG - Executing query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/Desktop/Computer Science/Mental_Health_Project/analysis/load_data.py:54: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "INFO:analysis.load_data:Loaded 1227 rows\n"
     ]
    }
   ],
   "source": [
    "tech_survey_df = load_dataset('tech_survey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af64fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:analysis.load_data:SQL file not found: /Users/Andrew/Desktop/Computer Science/Mental_Health_Project/pipeline/snowflake/static/tech_survey_extract.sql\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - PROJECT_ROOT: /Users/Andrew/Desktop/Computer Science/Mental_Health_Project\n",
      "DEBUG - SQL_PATHS[tech_survey]: pipeline/snowflake/static/tech_survey_extract.sql\n",
      "DEBUG - Resolved sql_path: /Users/Andrew/Desktop/Computer Science/Mental_Health_Project/pipeline/snowflake/static/tech_survey_extract.sql\n",
      "DEBUG - File exists: True\n",
      "DEBUG - Is file: True\n",
      "Return type: <class 'NoneType'>\n",
      "Return value: None\n",
      "Data not found\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tech_survey_df:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtech_survey_df\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m survey responses\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(tech_survey_df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m tech_survey_df.head()\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Load tech survey data\n",
    "tech_survey_df = load_dataset('tech_survey')\n",
    "print(f\"Return type: {type(tech_survey_df)}\")\n",
    "print(f\"Return value: {tech_survey_df}\")\n",
    "if not tech_survey_df:\n",
    "    print(\"Data not found\")\n",
    "print(f\"Loaded {len(tech_survey_df)} survey responses\")\n",
    "print(f\"Columns: {list(tech_survey_df.columns)}\")\n",
    "tech_survey_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "print(\"Data types:\")\n",
    "print(tech_survey_df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(tech_survey_df.isnull().sum())\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in tech_survey_df.columns:\n",
    "    print(f\"{col}: {tech_survey_df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff66500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for clustering\n",
    "config = CLUSTERING_CONFIG['preprocessing']\n",
    "categorical_cols = [col for col in config['categorical_columns'] if col in tech_survey_df.columns]\n",
    "numeric_cols = [col for col in config['numeric_columns'] if col in tech_survey_df.columns]\n",
    "\n",
    "print(f\"Categorical features: {categorical_cols}\")\n",
    "print(f\"Numeric features: {numeric_cols}\")\n",
    "print(f\"Total features for clustering: {len(categorical_cols + numeric_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for clustering\n",
    "features_df, label_encoders, scaler = prepare_clustering_features(\n",
    "    tech_survey_df, \n",
    "    categorical_cols, \n",
    "    numeric_cols\n",
    ")\n",
    "\n",
    "print(f\"Features prepared: {features_df.shape}\")\n",
    "print(f\"Sample after preprocessing:\")\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gower distance\n",
    "categorical_indices = list(range(len(categorical_cols)))\n",
    "distance_matrix = compute_gower_distance(features_df, categorical_indices)\n",
    "\n",
    "print(f\"Distance matrix shape: {distance_matrix.shape}\")\n",
    "print(f\"Distance matrix sample (first 5x5):\")\n",
    "print(distance_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d13353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HDBSCAN clustering\n",
    "clustering_config = CLUSTERING_CONFIG['hdbscan']\n",
    "\n",
    "clusterer, cluster_labels = run_hdbscan_clustering(\n",
    "    distance_matrix,\n",
    "    min_cluster_size=clustering_config['min_cluster_size'],\n",
    "    min_samples=clustering_config['min_samples']\n",
    ")\n",
    "\n",
    "# Check clustering results\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "print(f\"Cluster labels distribution:\")\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {label}: {count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering quality\n",
    "if n_clusters >= 2:\n",
    "    silhouette_avg = evaluate_clustering(distance_matrix, cluster_labels)\n",
    "    print(f\"Average silhouette score: {silhouette_avg:.3f}\")\n",
    "    \n",
    "    # Cluster stability score from HDBSCAN\n",
    "    print(f\"Cluster persistence scores: {clusterer.cluster_persistence_}\")\n",
    "else:\n",
    "    print(\"Insufficient clusters for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "if n_clusters >= 1:\n",
    "    # Add cluster labels to original dataframe\n",
    "    clustered_df = tech_survey_df.copy()\n",
    "    clustered_df['cluster'] = cluster_labels\n",
    "    \n",
    "    # Plot cluster sizes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "    cluster_counts.plot(kind='bar')\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.xlabel('Cluster Label (-1 = Noise)')\n",
    "    plt.ylabel('Number of Points')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustering results\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models/saved_models', exist_ok=True)\n",
    "\n",
    "# Save the trained clusterer\n",
    "model_filename = f\"../models/saved_models/hdbscan_clusterer_{datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(clusterer, f)\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "preprocessing_artifacts = {\n",
    "    'label_encoders': label_encoders,\n",
    "    'scaler': scaler,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'numeric_cols': numeric_cols,\n",
    "    'feature_columns': list(features_df.columns)\n",
    "}\n",
    "\n",
    "artifacts_filename = f\"../models/saved_models/clustering_preprocessing_{datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "with open(artifacts_filename, 'wb') as f:\n",
    "    pickle.dump(preprocessing_artifacts, f)\n",
    "\n",
    "print(f\"Model saved to: {model_filename}\")\n",
    "print(f\"Preprocessing artifacts saved to: {artifacts_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for analysis\n",
    "\n",
    "# Save clustered dataframe\n",
    "results_df = tech_survey_df.copy()\n",
    "results_df['cluster'] = cluster_labels\n",
    "\n",
    "output_filename = f\"../outputs/results/tech_survey_clustered_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "os.makedirs('../outputs/results', exist_ok=True)\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Clustered results saved to: {output_filename}\")\n",
    "print(f\"Ready for cluster profiling analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
