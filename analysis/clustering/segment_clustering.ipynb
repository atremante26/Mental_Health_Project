{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529f1e5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m     10\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mload_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prepare_clustering_features, compute_gower_distance\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclustering\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_hdbscan_clustering, evaluate_clustering\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Computer Science/Mental_Health_Project/analysis/clustering/../load_data.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpipeline\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnowflake\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload_snowflake\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m snowflake_connection\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_paths\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQL_PATHS\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Configure logging\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pipeline'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "sys.path.append('..')\n",
    "\n",
    "from load_data import load_dataset\n",
    "from utils.preprocessing import prepare_clustering_features, compute_gower_distance\n",
    "from clustering.clustering_utils import run_hdbscan_clustering, evaluate_clustering\n",
    "from config.model_config import CLUSTERING_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tech survey data\n",
    "tech_survey_df = load_dataset('tech_survey')\n",
    "print(f\"Loaded {len(tech_survey_df)} survey responses\")\n",
    "print(f\"Columns: {list(tech_survey_df.columns)}\")\n",
    "tech_survey_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "print(\"Data types:\")\n",
    "print(tech_survey_df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(tech_survey_df.isnull().sum())\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in tech_survey_df.columns:\n",
    "    print(f\"{col}: {tech_survey_df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff66500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for clustering\n",
    "config = CLUSTERING_CONFIG['preprocessing']\n",
    "categorical_cols = [col for col in config['categorical_columns'] if col in tech_survey_df.columns]\n",
    "numeric_cols = [col for col in config['numeric_columns'] if col in tech_survey_df.columns]\n",
    "\n",
    "print(f\"Categorical features: {categorical_cols}\")\n",
    "print(f\"Numeric features: {numeric_cols}\")\n",
    "print(f\"Total features for clustering: {len(categorical_cols + numeric_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for clustering\n",
    "features_df, label_encoders, scaler = prepare_clustering_features(\n",
    "    tech_survey_df, \n",
    "    categorical_cols, \n",
    "    numeric_cols\n",
    ")\n",
    "\n",
    "print(f\"Features prepared: {features_df.shape}\")\n",
    "print(f\"Sample after preprocessing:\")\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gower distance\n",
    "categorical_indices = list(range(len(categorical_cols)))\n",
    "distance_matrix = compute_gower_distance(features_df, categorical_indices)\n",
    "\n",
    "print(f\"Distance matrix shape: {distance_matrix.shape}\")\n",
    "print(f\"Distance matrix sample (first 5x5):\")\n",
    "print(distance_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d13353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HDBSCAN clustering\n",
    "clustering_config = CLUSTERING_CONFIG['hdbscan']\n",
    "\n",
    "clusterer, cluster_labels = run_hdbscan_clustering(\n",
    "    distance_matrix,\n",
    "    min_cluster_size=clustering_config['min_cluster_size'],\n",
    "    min_samples=clustering_config['min_samples']\n",
    ")\n",
    "\n",
    "# Check clustering results\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "print(f\"Cluster labels distribution:\")\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {label}: {count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering quality\n",
    "if n_clusters >= 2:\n",
    "    silhouette_avg = evaluate_clustering(distance_matrix, cluster_labels)\n",
    "    print(f\"Average silhouette score: {silhouette_avg:.3f}\")\n",
    "    \n",
    "    # Cluster stability score from HDBSCAN\n",
    "    print(f\"Cluster persistence scores: {clusterer.cluster_persistence_}\")\n",
    "else:\n",
    "    print(\"Insufficient clusters for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "if n_clusters >= 1:\n",
    "    # Add cluster labels to original dataframe\n",
    "    clustered_df = tech_survey_df.copy()\n",
    "    clustered_df['cluster'] = cluster_labels\n",
    "    \n",
    "    # Plot cluster sizes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "    cluster_counts.plot(kind='bar')\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.xlabel('Cluster Label (-1 = Noise)')\n",
    "    plt.ylabel('Number of Points')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustering results\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models/saved_models', exist_ok=True)\n",
    "\n",
    "# Save the trained clusterer\n",
    "model_filename = f\"../models/saved_models/hdbscan_clusterer_{datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(clusterer, f)\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "preprocessing_artifacts = {\n",
    "    'label_encoders': label_encoders,\n",
    "    'scaler': scaler,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'numeric_cols': numeric_cols,\n",
    "    'feature_columns': list(features_df.columns)\n",
    "}\n",
    "\n",
    "artifacts_filename = f\"../models/saved_models/clustering_preprocessing_{datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "with open(artifacts_filename, 'wb') as f:\n",
    "    pickle.dump(preprocessing_artifacts, f)\n",
    "\n",
    "print(f\"Model saved to: {model_filename}\")\n",
    "print(f\"Preprocessing artifacts saved to: {artifacts_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for analysis\n",
    "\n",
    "# Save clustered dataframe\n",
    "results_df = tech_survey_df.copy()\n",
    "results_df['cluster'] = cluster_labels\n",
    "\n",
    "output_filename = f\"../outputs/results/tech_survey_clustered_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "os.makedirs('../outputs/results', exist_ok=True)\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Clustered results saved to: {output_filename}\")\n",
    "print(f\"Ready for cluster profiling analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
